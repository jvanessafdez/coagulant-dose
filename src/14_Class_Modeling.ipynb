{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a08498f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install skops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ffbcb53a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tpot import TPOTClassifier\n",
    "# from imblearn.ensemble import BalancedBaggingClassifier\n",
    "# from sklearn.ensemble import ExtraTreesClassifier\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7641c2b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.decomposition import KernelPCA\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier, GradientBoostingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import learning_curve\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    " \n",
    "from pylab import rcParams\n",
    "from imblearn.pipeline import Pipeline\n",
    "import statsmodels.api as sm\n",
    "\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.model_selection import cross_val_score, GridSearchCV\n",
    "\n",
    "# import skops.io as sio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "290fe9d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0.2\n"
     ]
    }
   ],
   "source": [
    "import sklearn\n",
    "\n",
    "print(sklearn.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ac28cd64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.4.4\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "print(pd.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "28e1604b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python version: 3.9.13 (main, Aug 25 2022, 23:51:50) [MSC v.1916 64 bit (AMD64)]\n",
      "Pickle version (protocol): 5\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import pickle\n",
    "\n",
    "print(\"Python version:\", sys.version)\n",
    "print(\"Pickle version (protocol):\", pickle.HIGHEST_PROTOCOL)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "990587c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/df_category.csv',sep=',', encoding='latin-1', index_col=0, low_memory=False)\n",
    "df_edit = pd.read_csv('../data_edit/df_edit_category.csv',sep=',', encoding='latin-1', index_col=0, low_memory=False)\n",
    "df_2017 = pd.read_csv('../data/df_2017_category.csv',sep=',', encoding='latin-1', index_col=0, low_memory=False)\n",
    "df_edit_2017 = pd.read_csv('../data_edit/df_edit_2017_category.csv',sep=',', encoding='latin-1', index_col=0, low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c5b99e5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_without = pd.read_csv('../data/df_category_without_balance.csv',sep=',', encoding='latin-1', index_col=0, low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c92c6e27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def histogram(df, name):\n",
    "#     # Orden específico de las categorías\n",
    "#     orden = ['quince', 'veinte', 'veinticinco', 'treinta', 'treinta y cinco', 'cuarenta', 'cuarenta y cinco', 'cincuenta']\n",
    "    \n",
    "#     plt.figure(figsize=(10,6))\n",
    "#     # Crear el histograma\n",
    "#     df['dosis_coagulante'].value_counts().reindex(orden).plot(kind='bar')\n",
    "\n",
    "#     # Personalizar el gráfico\n",
    "#     plt.title(f'Cantidad de datos para cada categoría de dosis de coagulante de la\\n{name}')\n",
    "#     plt.xlabel('Categorías')\n",
    "#     plt.ylabel('Frecuencia')\n",
    "#     plt.xticks(rotation=45)  # Rota las etiquetas del eje x si es necesario\n",
    "\n",
    "#     # Mostrar el gráfico\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9514532e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# histogram(df, 'base de datos sin edición del 2013-2022.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f76e9577",
   "metadata": {},
   "outputs": [],
   "source": [
    "# histogram(df_edit, 'base de datos con edición del 2013-2022.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f6351dd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# histogram(df_2017, 'base de datos sin edición del 2017-2022.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7f99f0aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# histogram(df_edit_2017, 'base de datos con edición del 2013-2022.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "25822630",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def run_other_algorithms(dataframes, target_col='dosis_coagulante'):\n",
    "#     base_estimators = [RandomForestClassifier(), ExtraTreesClassifier(), KNeighborsClassifier(), \n",
    "#                        LogisticRegression(max_iter=1000),DecisionTreeClassifier(), GradientBoostingClassifier()]\n",
    "#     base_estimators_names = ['Random Forest', 'Extra Trees', 'K-Nearest Neighbors', 'Logistic Regression',\n",
    "#                              'Decision Tree', 'Gradient Boosting']\n",
    "\n",
    "#     for i, df in enumerate(dataframes):\n",
    "#         print(f\"\\nTrabajando con el dataframe número {i+1}\")\n",
    "        \n",
    "#         # Preparar datos\n",
    "#         y = df[target_col]\n",
    "#         X = df.drop(target_col, axis=1)\n",
    "#         X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.7)\n",
    "\n",
    "#         # Ejecutar modelos\n",
    "#         for estimator, name in zip(base_estimators, base_estimators_names):\n",
    "#             print(f\"\\nEjecutando algoritmo {name}\")\n",
    "#             a_est = estimator\n",
    "#             a_est.fit(X_train, y_train)\n",
    "#             pred_y = a_est.predict(X_test)\n",
    "#             mostrar_resultados(y_test, pred_y)\n",
    "\n",
    "# def mostrar_resultados(y_test, pred_y):\n",
    "#     conf_matrix = confusion_matrix(y_test, pred_y)\n",
    "#     LABELS = np.unique(y_test)\n",
    "#     plt.figure(figsize=(12, 12))\n",
    "#     sns.heatmap(conf_matrix, xticklabels=LABELS, yticklabels=LABELS, annot=True, fmt=\"d\");\n",
    "#     plt.title(\"Confusion matrix\")\n",
    "#     plt.ylabel('True class')\n",
    "#     plt.xlabel('Predicted class')\n",
    "#     plt.show()\n",
    "#     print (classification_report(y_test, pred_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "16a064eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataframes = [df, df_edit, df_2017, df_edit_2017]\n",
    "# run_other_algorithms(dataframes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b5da5f5",
   "metadata": {},
   "source": [
    "## Prueba de sobreajuste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0acd081a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def run_test(dataframes, target_col='dosis_coagulante'):\n",
    "#     base_estimators = [RandomForestClassifier(), ExtraTreesClassifier(), KNeighborsClassifier(), \n",
    "#                        LogisticRegression(max_iter=1000), DecisionTreeClassifier(), GradientBoostingClassifier()]\n",
    "#     base_estimators_names = ['Random Forest', 'Extra Trees', 'K-Nearest Neighbors', 'Logistic Regression',\n",
    "#                              'Decision Tree', 'Gradient Boosting']\n",
    "\n",
    "#     for i, df in enumerate(dataframes):\n",
    "#         print(f\"\\nTrabajando con el dataframe número {i+1}\")\n",
    "        \n",
    "#         # Preparar datos\n",
    "#         y = df[target_col]\n",
    "#         X = df.drop(target_col, axis=1)\n",
    "#         X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.7)\n",
    "\n",
    "#         # Ejecutar modelos\n",
    "#         for estimator, name in zip(base_estimators, base_estimators_names):\n",
    "#             print(f\"\\nEjecutando algoritmo {name}\")\n",
    "#             a_est = estimator\n",
    "#             a_est.fit(X_train, y_train)\n",
    "\n",
    "#             # Predicciones en el conjunto de entrenamiento\n",
    "#             train_preds = a_est.predict(X_train)\n",
    "#             # Predicciones en el conjunto de prueba\n",
    "#             test_preds = a_est.predict(X_test)\n",
    "            \n",
    "#             print(\"Informe de clasificación para el conjunto de entrenamiento:\")\n",
    "#             mostrar_resultados(y_train, train_preds)\n",
    "\n",
    "#             print(\"Informe de clasificación para el conjunto de prueba:\")\n",
    "#             mostrar_resultados(y_test, test_preds)\n",
    "\n",
    "# def mostrar_resultados(y_true, y_pred):\n",
    "#     conf_matrix = confusion_matrix(y_true, y_pred)\n",
    "#     LABELS = np.unique(y_true)\n",
    "#     plt.figure(figsize=(12, 12))\n",
    "#     sns.heatmap(conf_matrix, xticklabels=LABELS, yticklabels=LABELS, annot=True, fmt=\"d\");\n",
    "#     plt.title(\"Confusion matrix\")\n",
    "#     plt.ylabel('True class')\n",
    "#     plt.xlabel('Predicted class')\n",
    "#     plt.show()\n",
    "#     print (classification_report(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "817d3efb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataframes = [df, df_edit, df_2017, df_edit_2017]\n",
    "# run_test(dataframes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b3118240",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataframes = [df_without]\n",
    "# run_test(dataframes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64b67c67",
   "metadata": {},
   "source": [
    "Tras un cuidadoso análisis de los resultados, se ha determinado que los algoritmos de Random Forest y Extra Trees ofrecen el rendimiento más óptimo para los conjuntos de datos disponibles. En consecuencia, procederemos a realizar una evaluación más detallada de estos algoritmos, aplicando ajustes específicos a los hiperparámetros para cada conjunto de datos, con la finalidad de disminuir el sobreajuste de los modelos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c64e5b12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def train_and_evaluate_rf(df, n_estimators, max_depth, target='dosis_coagulante'):\n",
    "#     # Definir X e y\n",
    "#     y = df[target]\n",
    "#     X = df.drop(target, axis=1)\n",
    "\n",
    "#     # Divide tus datos en conjuntos de entrenamiento y prueba\n",
    "#     X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "#     # Crea un nuevo clasificador con los mejores hiperparámetros\n",
    "#     clf = make_pipeline(RandomForestClassifier(n_estimators=n_estimators, max_depth=max_depth, random_state=42))\n",
    "\n",
    "#     # Entrena el clasificador en el conjunto de entrenamiento\n",
    "#     clf.fit(X_train, y_train)\n",
    "\n",
    "#     # Prueba el clasificador en el conjunto de prueba\n",
    "#     y_pred = clf.predict(X_test)\n",
    "\n",
    "#     # Predicciones en el conjunto de entrenamiento\n",
    "#     train_preds = clf.predict(X_train)\n",
    "\n",
    "#     # Imprime los informes de clasificación para el entrenamiento y prueba\n",
    "#     print(\"Informe de clasificación para el conjunto de entrenamiento:\")\n",
    "#     print(classification_report(y_train, train_preds))\n",
    "    \n",
    "#     print(\"Informe de clasificación para el conjunto de prueba:\")\n",
    "#     print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1c47d930",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def train_and_evaluate_et(df, n_estimators, max_depth, min_samples_split, target='dosis_coagulante'):\n",
    "#     # Definir X e y\n",
    "#     y = df[target]\n",
    "#     X = df.drop(target, axis=1)\n",
    "\n",
    "#     # Divide tus datos en conjuntos de entrenamiento y prueba\n",
    "#     X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "#     # Crea un nuevo clasificador con los mejores hiperparámetros\n",
    "#     clf = make_pipeline(ExtraTreesClassifier(n_estimators=n_estimators, max_depth=max_depth, min_samples_split=min_samples_split, random_state=42))\n",
    "\n",
    "#     # Entrena el clasificador en el conjunto de entrenamiento\n",
    "#     clf.fit(X_train, y_train)\n",
    "\n",
    "#     # Prueba el clasificador en el conjunto de prueba\n",
    "#     y_pred = clf.predict(X_test)\n",
    "\n",
    "#     # Predicciones en el conjunto de entrenamiento\n",
    "#     train_preds = clf.predict(X_train)\n",
    "\n",
    "#     # Imprime los informes de clasificación para el entrenamiento y prueba\n",
    "#     print(\"Informe de clasificación para el conjunto de entrenamiento:\")\n",
    "#     print(classification_report(y_train, train_preds))\n",
    "    \n",
    "#     print(\"Informe de clasificación para el conjunto de prueba:\")\n",
    "#     print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "eb330712",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def evaluate_model_cv_rf(df, n_estimators, max_depth , target_col='dosis_coagulante'):\n",
    "#     y = df[target_col]\n",
    "#     X = df.drop(target_col, axis=1)\n",
    "    \n",
    "#     # Crear un pipeline para estandarizar los datos y luego aplicar el clasificador\n",
    "#     clf = make_pipeline(RandomForestClassifier(n_estimators=n_estimators, max_depth=max_depth, random_state=42))\n",
    "\n",
    "#     # Realiza la validación cruzada\n",
    "#     scores = cross_val_score(clf, X, y, cv=5)  \n",
    "#     print(\"Scores de cada iteración de la validación cruzada: \", scores)\n",
    "#     print(\"Media de los scores: \", scores.mean())\n",
    "    \n",
    "#     recall_scores = cross_val_score(clf, X, y, cv=5, scoring='recall_weighted')\n",
    "#     print(\"Recall de cada iteración de la validación cruzada: \", recall_scores)\n",
    "#     print(\"Media de los recalls: \", recall_scores.mean())\n",
    "\n",
    "#     precision_scores = cross_val_score(clf, X, y, cv=5, scoring='precision_weighted')\n",
    "#     print(\"Precision ponderada de cada iteración de la validación cruzada: \", precision_scores)\n",
    "#     print(\"Media de las precisiones ponderadas: \", precision_scores.mean())\n",
    "\n",
    "#     f1_scores = cross_val_score(clf, X, y, cv=5, scoring='f1_weighted')\n",
    "#     print(\"F1-score de cada iteración de la validación cruzada: \", f1_scores)\n",
    "#     print(\"Media de los F1-scores: \", f1_scores.mean())\n",
    "\n",
    "#     accuracy_scores = cross_val_score(clf, X, y, cv=5, scoring='accuracy')\n",
    "#     print(\"Accuracy de cada iteración de la validación cruzada: \", accuracy_scores)\n",
    "#     print(\"Media de los accuracy: \", accuracy_scores.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "08d2a1c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def evaluate_model_cv_et(df, n_estimators, max_depth, min_samples_split, target_col='dosis_coagulante'):\n",
    "#     y = df[target_col]\n",
    "#     X = df.drop(target_col, axis=1)\n",
    "    \n",
    "#     # Crear un pipeline para estandarizar los datos y luego aplicar el clasificador\n",
    "#     clf = make_pipeline(ExtraTreesClassifier(n_estimators=n_estimators, max_depth=max_depth, min_samples_split=min_samples_split, random_state=42))\n",
    "\n",
    "#     # Realiza la validación cruzada\n",
    "#     scores = cross_val_score(clf, X, y, cv=5)  \n",
    "\n",
    "#     # Realiza la validación cruzada\n",
    "#     scores = cross_val_score(clf, X, y, cv=5)  \n",
    "#     print(\"Scores de cada iteración de la validación cruzada: \", scores)\n",
    "#     print(\"Media de los scores: \", scores.mean())\n",
    "    \n",
    "#     recall_scores = cross_val_score(clf, X, y, cv=5, scoring='recall_weighted')\n",
    "#     print(\"Recall de cada iteración de la validación cruzada: \", recall_scores)\n",
    "#     print(\"Media de los recalls: \", recall_scores.mean())\n",
    "\n",
    "#     precision_scores = cross_val_score(clf, X, y, cv=5, scoring='precision_weighted')\n",
    "#     print(\"Precision ponderada de cada iteración de la validación cruzada: \", precision_scores)\n",
    "#     print(\"Media de las precisiones ponderadas: \", precision_scores.mean())\n",
    "\n",
    "#     f1_scores = cross_val_score(clf, X, y, cv=5, scoring='f1_weighted')\n",
    "#     print(\"F1-score de cada iteración de la validación cruzada: \", f1_scores)\n",
    "#     print(\"Media de los F1-scores: \", f1_scores.mean())\n",
    "\n",
    "#     accuracy_scores = cross_val_score(clf, X, y, cv=5, scoring='accuracy')\n",
    "#     print(\"Accuracy de cada iteración de la validación cruzada: \", accuracy_scores)\n",
    "#     print(\"Media de los accuracy: \", accuracy_scores.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fba63d7e",
   "metadata": {},
   "source": [
    "## Dataframe Completo sin Edición"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34f315f6",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e94c4de7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uso de la función\n",
    "# train_and_evaluate_rf(df, 200, 15)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50d6f622",
   "metadata": {},
   "source": [
    "**Validación cruzada**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ec2f3373",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate_model_cv_rf(df, 200, 15)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f4db32d",
   "metadata": {},
   "source": [
    "### Extra Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c849a13f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uso de la función\n",
    "# train_and_evaluate_et(df, 150, 25,5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd26ceaa",
   "metadata": {},
   "source": [
    "**Validación Cruzada**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "02f4636c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate_model_cv_et(df, 150, 25, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce34d9eb",
   "metadata": {},
   "source": [
    "### Análisis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a7534df",
   "metadata": {},
   "source": [
    "- **Rendimiento en el entrenamiento:** Extra Trees tiene un rendimiento ligeramente mejor en el conjunto de entrenamiento con una precisión del 90% en comparación con el 89% de Random Forest.\n",
    "\n",
    "- **Rendimiento en la prueba:** Random Forest tiene un rendimiento ligeramente mejor en el conjunto de pruebas con una precisión del 79% en comparación con el 78% de Extra Trees.\n",
    "\n",
    "- **Validación Cruzada:** Ambos modelos tienen rendimientos similares en la validación cruzada, lo que significa que su rendimiento es bastante consistente a través de diferentes divisiones de los datos.\n",
    "\n",
    "En este caso se valora más el rendimiento en datos no vistos (conjunto de pruebas). Por lo tanto, Random Forest podría ser una mejor opción."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b147094b",
   "metadata": {},
   "source": [
    "## Dataframe Completo con Edición"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5beccd34",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "10b6c435",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uso de la función\n",
    "# train_and_evaluate_rf(df_edit, 150, 13)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "491641bb",
   "metadata": {},
   "source": [
    "**Validación cruzada**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "25a9e40e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate_model_cv_rf(df_edit, 150, 13)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe074d33",
   "metadata": {},
   "source": [
    "### Extra Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4c22dd32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uso de la función\n",
    "# train_and_evaluate_et(df_edit, 150, 22, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d58e206e",
   "metadata": {},
   "source": [
    "**Validación Cruzada**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f878f82e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate_model_cv_et(df_edit, 150, 22, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5539744",
   "metadata": {},
   "source": [
    "### Análisis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4b49f37",
   "metadata": {},
   "source": [
    "Mirando los resultados de ambos modelos, Random Forest y Extra Trees, podemos realizar un análisis en varios aspectos:\n",
    "\n",
    "- **Precisión general (Accuracy):** Ambos modelos muestran un nivel de precisión similar en el conjunto de entrenamiento con alrededor del 83%. Sin embargo, en el conjunto de prueba, Random Forest tiene un rendimiento ligeramente mejor con una precisión del 72%, mientras que Extra Trees muestra una precisión del 70%.\n",
    "\n",
    "- **Precisión y Recall por clase:** En el conjunto de entrenamiento, Extra Trees parece tener una mejor precisión para las clases 'cincuenta', 'cuarenta', 'cuarenta y cinco', 'quince', 'treinta' y 'treinta y cinco', pero Random Forest tiene un mejor rendimiento en 'veinte' y 'veinticinco'. Para el conjunto de prueba, vemos una situación similar, aunque las diferencias son menos pronunciadas. Notablemente, Extra Trees parece tener una mayor capacidad para predecir correctamente la clase 'cincuenta' con una precisión y recall altos.\n",
    "\n",
    "- **F1-Score:** En el conjunto de entrenamiento, ambos modelos muestran un F1-Score promedio similar. Sin embargo, en el conjunto de prueba, Random Forest tiene un rendimiento ligeramente mejor con un F1-Score promedio de 0.71 frente al 0.70 de Extra Trees.\n",
    "\n",
    "- **Validación Cruzada:** Los resultados de la validación cruzada (Cross-Validation) para Random Forest son consistentemente más altos que para Extra Trees, tanto en términos de precisión (score), F1-Score como recall, sugiriendo que este modelo es más robusto y menos propenso a sobreajuste.\n",
    "\n",
    "Considerando estos factores, parecería que Random Forest es el modelo superior. Sin embargo, vale la pena señalar que las diferencias entre los dos modelos no son abrumadoras. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43208a2e",
   "metadata": {},
   "source": [
    "## Dataframe 2017-2022 sin Edición"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ba8ea99",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "bc551294",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uso de la función\n",
    "# train_and_evaluate_rf(df_2017, 200, 17)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9538b4c1",
   "metadata": {},
   "source": [
    "**Validación Cruzada**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f7f459e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate_model_cv_rf(df_2017, 200, 17)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5780feb",
   "metadata": {},
   "source": [
    "### Extra Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5fb75fc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uso de la función\n",
    "# train_and_evaluate_et(df_2017, 150, 25, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc793348",
   "metadata": {},
   "source": [
    "**Validación Cruzada**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "99ea0188",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate_model_cv_et(df_2017, 150, 25, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c05a5230",
   "metadata": {},
   "source": [
    "### Análisis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3d6161a",
   "metadata": {},
   "source": [
    "Ambos modelos, Random Forest y Extra Trees, muestran un rendimiento sólido. Sin embargo, en general, Random Forest parece superar ligeramente a Extra Trees en términos de métricas de rendimiento. Específicamente:\n",
    "\n",
    "La precisión ponderada en los conjuntos de entrenamiento y prueba es ligeramente mayor en Random Forest.\n",
    "Las medias de validación cruzada para scores, F1-scores, y recalls son también un poco mejores en Random Forest.\n",
    "Por lo tanto, mi criterio me inclina a seleccionar el Random Forest como el mejor modelo en este caso, debido a su rendimiento generalmente más fuerte en las métricas clave de clasificación. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71991998",
   "metadata": {},
   "source": [
    "## Dataframe 2017-2022 con Edición"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f27ee9e6",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5ba3e369",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uso de la función\n",
    "# train_and_evaluate_rf(df_edit_2017, 200, 15)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7becba0",
   "metadata": {},
   "source": [
    "**Validación Cruzada**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "af86eea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate_model_cv_rf(df_edit_2017, 200, 17)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d9b84c0",
   "metadata": {},
   "source": [
    "### Extra Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4bc49e72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Uso de la función\n",
    "# train_and_evaluate_et(df_edit_2017, 150, 25, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c3232e3",
   "metadata": {},
   "source": [
    "**Validación Cruzada**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "33fcf397",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate_model_cv_et(df_edit_2017, 150, 25, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fd9b5eb",
   "metadata": {},
   "source": [
    "### Análisis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3d0a1fa",
   "metadata": {},
   "source": [
    "Los resultados de ambos modelos son bastante similares. Sin embargo, el modelo Random Forest muestra una mejor exactitud en ambos conjuntos de entrenamiento y prueba. Además, el modelo Random Forest también tiene una media de los scores de validación cruzada levemente superior.\n",
    "\n",
    "Además de la exactitud y los scores de validación cruzada, el F1-score es otra métrica importante que debemos considerar. Es una métrica que combina precisión y recall, y puede ser una mejor métrica cuando las clases están desbalanceadas. El F1-score medio del modelo Random Forest también es superior al del modelo Extra Trees.\n",
    "\n",
    "Por lo tanto, basándonos en estos resultados, podemos concluir que el modelo Random Forest del conjunto del 2017-2022 con Edición parece ser la mejor opción."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "466ff5c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# y = df_edit_2017['dosis_coagulante']\n",
    "# X = df_edit_2017.drop('dosis_coagulante', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "32e99de5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "7629108e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def plot_learning_curve(estimator, title, X, y, ylim=None, cv=None, n_jobs=None, train_sizes=np.linspace(.1, 1.0, 5)):\n",
    "\n",
    "#     plt.figure()\n",
    "#     plt.title(title)\n",
    "#     if ylim is not None:\n",
    "#         plt.ylim(*ylim)\n",
    "#     plt.xlabel(\"Training examples\")\n",
    "#     plt.ylabel(\"Score\")\n",
    "    \n",
    "#     train_sizes, train_scores, test_scores = learning_curve(estimator, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes)\n",
    "\n",
    "#     train_scores_mean = np.mean(train_scores, axis=1)\n",
    "#     train_scores_std = np.std(train_scores, axis=1)\n",
    "#     test_scores_mean = np.mean(test_scores, axis=1)\n",
    "#     test_scores_std = np.std(test_scores, axis=1)\n",
    "    \n",
    "#     plt.grid()\n",
    "\n",
    "#     plt.fill_between(train_sizes, train_scores_mean - train_scores_std, train_scores_mean + train_scores_std, alpha=0.1, color=\"r\")\n",
    "#     plt.fill_between(train_sizes, test_scores_mean - test_scores_std, test_scores_mean + test_scores_std, alpha=0.1, color=\"g\")\n",
    "#     plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\", label=\"Training score\")\n",
    "#     plt.plot(train_sizes, test_scores_mean, 'o-', color=\"g\", label=\"Cross-validation score\")\n",
    "\n",
    "#     plt.legend(loc=\"best\")\n",
    "#     return plt\n",
    "\n",
    "# cv = ShuffleSplit(n_splits=30, test_size=0.2, random_state=0)\n",
    "\n",
    "# estimator = RandomForestClassifier(n_estimators=200, max_depth=17, random_state=42)\n",
    "\n",
    "# plot_learning_curve(estimator, \"Curvas de aprendizaje (Random Forest)\", X, y, ylim=(0.6, 1.01), cv=cv, n_jobs=-1)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7891ea88",
   "metadata": {},
   "source": [
    "Con este modelo se hará una pequeña evaluación para ver que tanta proximidad a los valores reales se tiene."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c338fa54",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d587497e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def plot_learning_curve_2(estimator, title, X, y, ylim=None, cv=None, n_jobs=None, train_sizes=np.linspace(.1, 1.0, 5)):\n",
    "\n",
    "#     plt.figure()\n",
    "#     plt.title(title)\n",
    "#     if ylim is not None:\n",
    "#         plt.ylim(*ylim)\n",
    "#     plt.xlabel(\"Training examples\")\n",
    "#     plt.ylabel(\"Score\")\n",
    "    \n",
    "#     train_sizes, train_scores, test_scores = learning_curve(estimator, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes)\n",
    "\n",
    "#     train_scores_mean = np.mean(train_scores, axis=1)\n",
    "#     train_scores_std = np.std(train_scores, axis=1)\n",
    "#     test_scores_mean = np.mean(test_scores, axis=1)\n",
    "#     test_scores_std = np.std(test_scores, axis=1)\n",
    "    \n",
    "#     plt.grid()\n",
    "\n",
    "#     plt.fill_between(train_sizes, train_scores_mean - train_scores_std, train_scores_mean + train_scores_std, alpha=0.1, color=\"r\")\n",
    "#     plt.fill_between(train_sizes, test_scores_mean - test_scores_std, test_scores_mean + test_scores_std, alpha=0.1, color=\"g\")\n",
    "#     plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\", label=\"Training score\")\n",
    "#     plt.plot(train_sizes, test_scores_mean, 'o-', color=\"g\", label=\"Cross-validation score\")\n",
    "\n",
    "#     plt.legend(loc=\"best\")\n",
    "#     return plt\n",
    "\n",
    "# cv = ShuffleSplit(n_splits=30, test_size=0.2, random_state=0)\n",
    "\n",
    "# estimator = ExtraTreesClassifier(n_estimators=150, max_depth=25, min_samples_split=5, random_state=42)\n",
    "\n",
    "# plot_learning_curve_2(estimator, \"Learning Curves (Extra Trees)\", X, y, ylim=(0.6, 1.01), cv=cv, n_jobs=-1)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "5e5ed4ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# y = df_2017['dosis_coagulante']\n",
    "# X = df_2017.drop('dosis_coagulante', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "fad5e1ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Definir los valores de los hiperparámetros que quieres probar\n",
    "# param_grid = {'n_estimators': np.arange(100, 250, 50),\n",
    "#               'max_depth': np.arange(10, 30, 5),\n",
    "#               'min_samples_split': np.arange(2, 10, 2),\n",
    "#              }\n",
    "\n",
    "# # Realiza la búsqueda en cuadrícula con validación cruzada\n",
    "# grid_search = GridSearchCV(RandomForestClassifier(), param_grid, cv=5)\n",
    "# grid_search.fit(X, y)\n",
    "\n",
    "# # Imprime la mejor combinación de hiperparámetros\n",
    "# print(\"Mejores hiperparámetros: \", grid_search.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "9ba4e2c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define el número de componentes de PCA\n",
    "# n_components = 6\n",
    "\n",
    "# # Crea un nuevo clasificador con los mejores hiperparámetros\n",
    "# clf = make_pipeline(PCA(n_components=n_components), RandomForestClassifier(n_estimators=200, max_depth=15, random_state=42))\n",
    "\n",
    "# # Entrena el clasificador en el conjunto de entrenamiento\n",
    "# clf.fit(X_train, y_train)\n",
    "\n",
    "# # Prueba el clasificador en el conjunto de prueba\n",
    "# y_pred = clf.predict(X_test)\n",
    "\n",
    "# # Predicciones en el conjunto de entrenamiento\n",
    "# train_preds = clf.predict(X_train)\n",
    "            \n",
    "# print(\"Informe de clasificación para el conjunto de entrenamiento:\")\n",
    "# print(classification_report(y_train, train_preds))\n",
    "\n",
    "# # Imprime un informe de clasificación\n",
    "# print(\"Informe de clasificación para el conjunto de prueba:\")\n",
    "# print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "89b80931",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sio.dump(clf, \"clf.skops\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "c6f31355",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sio.load(\"./clf.skops\", trusted=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "cb423334",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def plot_learning_curve(estimator, title, X, y, ylim=None, cv=None, n_jobs=None, train_sizes=np.linspace(.1, 1.0, 5)):\n",
    "\n",
    "#     plt.figure()\n",
    "#     plt.title(title)\n",
    "#     if ylim is not None:\n",
    "#         plt.ylim(*ylim)\n",
    "#     plt.xlabel(\"Training examples\")\n",
    "#     plt.ylabel(\"Score\")\n",
    "    \n",
    "#     train_sizes, train_scores, test_scores = learning_curve(estimator, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes)\n",
    "\n",
    "#     train_scores_mean = np.mean(train_scores, axis=1)\n",
    "#     train_scores_std = np.std(train_scores, axis=1)\n",
    "#     test_scores_mean = np.mean(test_scores, axis=1)\n",
    "#     test_scores_std = np.std(test_scores, axis=1)\n",
    "    \n",
    "#     plt.grid()\n",
    "\n",
    "#     plt.fill_between(train_sizes, train_scores_mean - train_scores_std, train_scores_mean + train_scores_std, alpha=0.1, color=\"r\")\n",
    "#     plt.fill_between(train_sizes, test_scores_mean - test_scores_std, test_scores_mean + test_scores_std, alpha=0.1, color=\"g\")\n",
    "#     plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\", label=\"Training score\")\n",
    "#     plt.plot(train_sizes, test_scores_mean, 'o-', color=\"g\", label=\"Cross-validation score\")\n",
    "\n",
    "#     plt.legend(loc=\"best\")\n",
    "#     return plt\n",
    "\n",
    "# cv = ShuffleSplit(n_splits=30, test_size=0.2, random_state=0)\n",
    "\n",
    "# estimator = RandomForestClassifier(n_estimators=200, max_depth=30, random_state=42)\n",
    "\n",
    "# plot_learning_curve(estimator, \"Learning Curves (Random Forest)\", X, y, ylim=(0.7, 1.01), cv=cv, n_jobs=-1)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "c814850c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(df, n_estimators, max_depth , target_col='dosis_coagulante'):\n",
    "    y = df[target_col]\n",
    "    X = df.drop(target_col, axis=1)\n",
    "    \n",
    "    # Dividir el conjunto de datos en entrenamiento y prueba\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Crear un pipeline para estandarizar los datos y luego aplicar el clasificador\n",
    "    clf = make_pipeline(RandomForestClassifier(n_estimators=n_estimators, max_depth=max_depth, random_state=42))\n",
    "    clf.fit(X_train, y_train)\n",
    "    \n",
    "    return clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "37ee8792",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model(df_edit_2017, 200, 17)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "951d77e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_and_display(model, input_df):\n",
    "    # Asegurarte de que las columnas del dataframe de entrada coinciden con las del modelo\n",
    "    expected_columns = ['agua_cruda_p_h', 'agua_cruda_color', 'agua_cruda_alcalinidad',\n",
    "                        'agua_cruda_conductividad', 'vel_viento', 'precipitacion',\n",
    "                        'temp_humeda', 'clasificador_mensual']\n",
    "    \n",
    "    if not all([col in input_df.columns for col in expected_columns]):\n",
    "        raise ValueError(f\"El DataFrame proporcionado debe contener las siguientes columnas: {expected_columns}\")\n",
    "    \n",
    "    # Usar el modelo para predecir\n",
    "    predictions = model.predict(input_df)\n",
    "    \n",
    "    # Agregar las predicciones al dataframe original para mostrar en una tabla\n",
    "    input_df['Predicción'] = predictions\n",
    "    \n",
    "    # Imprimir el dataframe resultante\n",
    "    print(input_df)\n",
    "    return input_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "cc5fd9f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\n",
    "    'agua_cruda_p_h': [7.1, 7.2, 7.3, 7.1, 7.3, 7.3, 7.4, 7.4, 7.3],\n",
    "    'agua_cruda_color': [32, 23, 23, 131, 20, 88, 17, 36, 33],\n",
    "    'agua_cruda_alcalinidad': [25, 24, 25, 31, 22, 22, 26, 28, 21],\n",
    "    'agua_cruda_conductividad': [49, 51.9, 48.3, 47.5, 51.4, 48.8, 55.8, 60.9, 47.3],\n",
    "    'vel_viento': [0.65, 0.908333333, 1, 0.633333333, 2.125, 1.625, 1.083333333, 1.691666666, 1.44166666],\n",
    "    'precipitacion': [0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "    'temp_humeda': [14, 17, 15, 15, 18, 16, 15, 17, 14],\n",
    "    'clasificador_mensual': [1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
    "}\n",
    "\n",
    "input_df = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "f24e91f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_df = predict_and_display(model, input_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "fbefe7b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Guardar el modelo a un archivo\n",
    "with open('rf_model.pkl', 'wb') as model_file:\n",
    "    pickle.dump(model, model_file)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
